{
    "units_first_layer": 400,
    "units_second_layer": 200,
    "activation": "relu", 
    "loss": "mse", 
    "learning_rate": 0.001,
    "epochs": 150,
    "batch_size": 64,
    "latent_dim": 6
}